# Asperathos's Quick Start

The purpose of this guide is to demonstrate how to easily setup all Asperathos components.

## 1. Requirements 

1. Have a Kubernetes cluster ready to receive applications;
2. Have an application image capable consuming items from a ```redis``` workload;
3. Configure a requirements file for your application.

## 2. Creating a Kubernetes cluster

1. The following [guide](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/) retrieved from the official Kubernetes documentation can be used to easily deploy a cluster. The ```config``` file generated by the end of their guide will be used in the following steps.

### 2.1. Configuring your cluster to support Intel SGX

If you want to execute SGX applications within your newly created K8s cluster, you need the following:

1. The machines in your cluster should comply with the Intel SGX hardware requirements [here](https://github.com/intel/linux-sgx-driver/blob/master/README.md);
2. Each node should have the SGX driver installed;

#### 2.1.1. Installing the SGX driver

1. Enter each node of your cluster;
2. You can install the SGX drivers following the [readme](https://github.com/intel/linux-sgx-driver/blob/master/README.md) available in the official repository.

### 2.2. Configuring your cluster to use SCONE applications

In order to execute SCONE applications in your cluster, you should have a Local Attestation Server (LAS) instance deployed in each of your nodes. LAS is a service provided by SCONE. To understand better what it is and what it does, you can read the official SCONE documentation [here](https://sconedocs.github.io/LASIntro/).

Note that the LAS instance can be a Kubernetes [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/).

This is an example of the manifest that you can use to deploy a LAS instance:
      
```
apiVersion: apps/v1
kind: DaemonSet
metadata:
   name: local-attestation
   labels:
      k8s-app: local-attestation
spec:
   selector:
      matchLabels:
         k8s-app: local-attestation
   updateStrategy:
      type: RollingUpdate
   template:
      metadata:
         labels:
            k8s-app: local-attestation
      spec:
         volumes:
         - name: dev-isgx
         hostPath:
            path: /dev/isgx
         containers:
            - name: local-attestation-svc
            image: <address_of_your_las_docker_image>
            tty: true
            volumeMounts:
            - mountPath: /dev/isgx
               name: dev-isgx
            securityContext:
               privileged: true
            ports:
            - containerPort: 18766
               hostPort: 18766
               protocol: TCP
         hostNetwork: true
```

1. In ```manifest.yaml``` change ```<address_of_your_las_docker_image>``` to the right address for your LAS image;

2. To deploy the LAS instance, run:

```bash
$ kubectl apply -f manifest.yaml
```

## 3. Creating an image to consume a KubeJobs workload

### 3.1. Creating a Python application that knows how to consume a redis workload:

The application must know how to use the ```redis``` API. As an example, take the following simple worker. It consumes an item from ```redis```, which is assumed to be an URL. The worker then downloads the content of the URL and prints to stdout.

In Python, ```application.py``` would look like this:

```python
import redis
import requests
import os

# Asperathos will return the redis host as an enviroment variable named REDIS_HOST.
# The default port of redis is 6379.
r = redis.StrictRedis(host=os.environ['REDIS_HOST'],
                      port=6379,
                      db=0)

# r.llen("job") returns the length of the queue "job" on redis.
while r.llen("job") > 0:

    # `rpoplpush` moves one item from our work queue
    # to an auxiliary queue for items being processed,
    # returning its value
    item_url = r.rpoplpush('job', 'job:processing')

    # downloading the content of the items
    content = requests.get(item_url).text

    # print to stdout
    print(content)
```

### 3.2. Creating a requirements file:

It is required to make a ```requirements.txt``` file with all necessary imports from the application. In this example the application needs the ```redis``` and ```requests``` modules. The file should look like this:

```
redis
requests
```

### 3.3. Creating a Dockerfile:

In the same directory as the application file is located, create a file named ```Dockerfile``` with the following content:

```Dockerfile
FROM python:2.7
COPY requirements.txt .
COPY application.py .
RUN pip install -r requirements.txt
```

### 3.4. Building, tagging and pushing the image:

Here we consider the name of the image to be ```quickstart``` and the tag ```demo```.

1. To build the image run the following command:
   
```bash
$ docker build -t quickstart:demo .
```

2. To create a tag run the following command:

```bash
$ docker tag quickstart:demo <repository>/quickstart:demo
```

The ```<repository>``` variable represents a public repository such as *dockerhub* or your private repository of choice. If private, it's also required to specify a port.

3. To push to the repository run the following command:

```bash
$ docker push <repository>/quickstart:demo
```

## 4. How to deploy an Asperathos instance?

Asperathos can be deployed as a simple compose application, following the steps bellow:

### 4.1. Install Docker and Docker Compose

```bash
$ curl https://get.docker.com/ | bash
$ apt-get install docker-compose
```

If you want, use the command bellow to grant root access to your user when using `docker` commands in your terminal:

```bash
$ sudo usermod -a -G docker $USER
```

After that, log in and out from your session to update the changes.
 
 ### 4.2. Clone Asperathos Compose repository

```bash
$ git clone https://github.com/ufcg-lsd/asperathos-compose
$ cd asperathos-compose
```

### 4.3. Set your KUBECONFIG env var

Remember the cluster we previously deployed in this tutorial? Get the config file generated there and set the following environment variable as follows:

```bash
$ export KUBECONFIG=/path/to/your/kube/config
```
### 4.4. Configuring the Asperathos components

If you wish, more information on how to customize your Asperathos can be found in our detailed User Guide file. In order to change such configurations you should edit the config files for each component available on ```controller.cfg```, ```manager.cfg```, ```monitor.cfg``` and ```visualizer.cfg``` in the *asperathos-compose* directory.

For a simple installation you can skip this step.

### 4.5. Build the images

```bash
$ ./build.sh
```

### 4.6. Run compose

```bash
$ docker-compose up -d
```

Done! Your Asperathos is up and running :)

### 4.7 Check the services

You should have 4 new containers running the Asperathos components. Check that by running:

```bash
$ docker ps
```

The response should look something like this:

```
CONTAINER ID        IMAGE                   NAMES
18bada9fd488        asperathos_monitor      asperathoscompose_monitor_1
8c6afad539b7        asperathos_visualizer   asperathoscompose_visualizer_1
4036dfweb209        asperathos_manager      asperathoscompose_manager_1
367979qe4623        asperathos_controller   asperathoscompose_controller_1
```

To check the logs of the Manager component, for instance, run:

```bash
$ docker logs -f asperathoscompose_manager_1
```

## 5. How to submit a KubeJobs job?

First, create a file named ```kubejobs-job.json```. The content of this file will have all the necessary commands to activate each plugin of each Asperathos component. Also, this JSON file also contains information about the user credentials, workload that will be running, image that will be used and so on. You can check below a template of how the ```kubejobs-job.json``` file should look like:

```json
{  
   "plugin":"kubejobs",
   "enable_auth":false,
   "plugin_info":{  
      "username": "user",
      "password": "psswrd",
      "cmd":[  
         ...
      ],
      "img": "img-url:port",
      "job_resource_lifetime": 300,
      "init_size": 1,      
      "redis_workload": "workload-url",
      "config_id": "id",
      "control_plugin": "kubejobs",
      "control_parameters": {
         "schedule_strategy": "default",    
         "max_size": 10,
         "actuator": "k8s_replicas",
         "check_interval": 5,
         "trigger_down": 0,
         "trigger_up": 0,
         "min_rep": 1,
         "max_rep": 10,
         "actuation_size": 1,
         "metric_source": "redis"
      },
      "monitor_plugin": "kubejobs",
      "monitor_info":{  
         "expected_time": 40
      },
      "enable_visualizer":true,
      "visualizer_plugin":"k8s-grafana",
      "visualizer_info":{  
         "datasource_type":"datasource-type"
      },
      "env_vars":{  
         [...]
      }
   }
}
```

* The **cmd** needs to contain the command that will launch the file that will consume the workload, in this tutorial we consider "python", "application.py"'.

* The **img-url:port** variable will be the image generated in the first tutorial of this guide.

* The **workload-url** is a URL to a text file containing all the work items, one per line. Asperathos pushes these values to a work queue which the workers will later consume. For our simple worker that just downloads the contents of a URL and prints them to stdout, consider the `workload-url` example [here](https://gist.githubusercontent.com/clenimar/9ecb14e2346af72763303a9957c94ea3/raw/1c014690f10c79bb9fe2165360ee92c106485ee7/workload-url.txt). It contains URLs to text files containing phrases that are going to be printed.

* The **datasource-type** will be 'influxdb' in this example.

* The **env_vars** variable needs to contain any environment variables that the file contained in the image generated in the first part of this guide will use.

2. Then, in a different terminal, run the following command to submit the job to the Manager component.

```bash
$ cd path/to/kubejobs.json
$ curl -H "Content-Type: application/json" --data @kubejobs-job.json http://0.0.0.0:1500/submissions
```

If your Asperathos is running somewhere other than locally, change its address in the endpoint URL.

## 6. How collect the visualization URL of a specific job?

1. After a job is launched, with the visualization flag setted to ‘enable’, it is possible to retrieve the visualization URL to keep track of the job progress at execution time. The following GET request to the Visualizer api can be used to retrieve this information. 

```bash
$ curl visualizer-ip:port/visualizing/id
```

Where:

* **visualizer-ip**: The IP where the visualizer service are running.
* **port**: The port where the visualizer service are answering from.
* **id**: The ID of the job launched.
